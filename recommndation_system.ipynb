{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_review = pd.read_csv('training-RestoInfo/RestoInfo.csv')\n",
    "restaurant_review = restaurant_review.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def replacenth(string, sub, wanted, n):\n",
    "    where = [m.start() for m in re.finditer(sub, string)][n-1]\n",
    "    before = string[:where]\n",
    "    after = string[where:]\n",
    "    after = after.replace(sub, wanted, 1)\n",
    "    newString = before + after\n",
    "    return newString\n",
    "\n",
    "def fix_remover_eval(remover):\n",
    "    bad_quotes = []\n",
    "    for i,match in enumerate(re.finditer(r\"\\'\", remover)):\n",
    "        span_start, span_end = match.span()\n",
    "        before = remover[span_start-2:span_start]\n",
    "        after = remover[span_end:span_end+1]\n",
    "        if before == '[(' or before == ' (' or before == ', ':\n",
    "            continue\n",
    "        if after == \",\" or after == \")\":\n",
    "            continue\n",
    "        else:\n",
    "            bad_quotes.append((i, match.group(0)))\n",
    "\n",
    "    for i,q in bad_quotes:\n",
    "        remover = replacenth(remover, \"\\'\", \"\\\\'\", i+1)\n",
    "\n",
    "    bad_quotes = []\n",
    "    for i,match in enumerate(re.finditer(r'\\\"', remover)):\n",
    "        span_start, span_end = match.span()\n",
    "        before = remover[span_start-2:span_start]\n",
    "        after = remover[span_end:span_end+1]\n",
    "        if before == '[(' or before == ' (' or before == ', ':\n",
    "            continue\n",
    "        if after == \",\" or after == \")\" or after == ', ':\n",
    "            continue\n",
    "        else:\n",
    "            bad_quotes.append((i, match.group(0)))\n",
    "\n",
    "    for i,q in bad_quotes:\n",
    "        remover = replacenth(remover, '\\\"', '\\\\\"', i+1)\n",
    "    return eval(remover)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eval(t):\n",
    "    try:\n",
    "        return fix_remover_eval(t)\n",
    "    except:\n",
    "        return t\n",
    "    \n",
    "restaurant_review['reviews_list_evaled'] = restaurant_review['reviews_list'].apply(get_eval)\n",
    "restaurant_review['menu_item_evaled'] =restaurant_review['menu_item'].apply(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "restaurant_review_fixed = restaurant_review[restaurant_review['reviews_list_evaled'].apply(type) != str].rename(\n",
    "    {\"Unnamed: 0\": \"rest_id\"}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_review_fixed['cuisines_list'] = restaurant_review_fixed[\n",
    "    'cuisines'].fillna(\"\").apply(lambda x: x.split(' ,'))\n",
    "restaurant_review_fixed['dish_liked_list'] = restaurant_review_fixed[\n",
    "    'dish_liked'].fillna(\"\").apply(lambda x: x.split(' ,'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendation_requirements = restaurant_review_fixed[['name',\n",
    "    'reviews_list_evaled', 'menu_item_evaled', 'cuisines_list', \n",
    "    'listed_in(city)', 'approx_cost(for two people)', 'rate', 'dish_liked_list']].rename(\n",
    "    {\"listed_in(city)\": \"location\", \"approx_cost(for two people)\": \"cost\"}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "def float_converter(x):\n",
    "    try:\n",
    "        all_floats = re.findall(r'\\d+', x)\n",
    "        if all_floats:\n",
    "            return float(all_floats[0])\n",
    "        else:\n",
    "            return -1\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "recommendation_requirements['cost'] = recommendation_requirements['cost'].fillna(\"-1\").apply(\n",
    "    lambda x: int(x.replace(\",\", \"\")))\n",
    "\n",
    "recommendation_requirements['rate'] = recommendation_requirements['rate'].apply(float_converter)\n",
    "\n",
    "recommendation_requirements = recommendation_requirements.drop_duplicates(['name', 'location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim.utils import tokenize\n",
    "\n",
    "def get_words_ordered(s):\n",
    "    if not s:\n",
    "        return []\n",
    "    return [w for w in tokenize(remove_stopwords(s.lower())) if w]\n",
    "\n",
    "\n",
    "def get_words(s):\n",
    "    if not s:\n",
    "        return set()\n",
    "    return set(get_words_ordered(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-649-fac03c469537>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     lambda x: x[1].strip('RATEDn  '))\n\u001b[1;32m      5\u001b[0m \u001b[0mall_reviews_tokenised\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_reviews\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_words_ordered\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_reviews_tokenised\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, iter, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, max_final_vocab)\u001b[0m\n\u001b[1;32m    598\u001b[0m             \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m             seed=seed, hs=hs, negative=negative, cbow_mean=cbow_mean, min_alpha=min_alpha, compute_loss=compute_loss)\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m     def _do_train_epoch(self, corpus_file, thread_id, offset, cython_vocab, thread_private_mem, cur_epoch,\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, workers, vector_size, epochs, callbacks, batch_words, trim_rule, sg, alpha, window, seed, hs, negative, ns_exponent, cbow_mean, min_alpha, compute_loss, **kwargs)\u001b[0m\n\u001b[1;32m    743\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You can't pass a generator as the sentences argument. Try a sequence.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 745\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    746\u001b[0m             self.train(\n\u001b[1;32m    747\u001b[0m                 \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mbuild_vocab\u001b[0;34m(self, sentences, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001b[0m\n\u001b[1;32m    925\u001b[0m         report_values = self.vocabulary.prepare_vocab(\n\u001b[1;32m    926\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_raw_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeep_raw_vocab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m             trim_rule=trim_rule, **kwargs)\n\u001b[0m\u001b[1;32m    928\u001b[0m         \u001b[0mreport_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'memory'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimate_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreport_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_retained_words'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mprepare_vocab\u001b[0;34m(self, hs, negative, wv, update, keep_raw_vocab, trim_rule, min_count, sample, dry_run)\u001b[0m\n\u001b[1;32m   1471\u001b[0m                     \u001b[0mretain_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1472\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdry_run\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1473\u001b[0;31m                         \u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1474\u001b[0m                         \u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1475\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \"\"\"\n\u001b[0;32m--> 203\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "all_reviews = recommendation_requirements['reviews_list_evaled'].explode().dropna().apply(\n",
    "    lambda x: x[1].strip('RATEDn  '))\n",
    "all_reviews_tokenised = all_reviews.apply(get_words_ordered).tolist()\n",
    "model = Word2Vec(all_reviews_tokenised, size=200, window=5, min_count=2, workers=8, iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_set_subset_standard(s1_set, s2):\n",
    "    s2_set = get_words(s2.lower())\n",
    "    return len(s1_set.intersection(s2_set))/len(s1_set)\n",
    "\n",
    "\n",
    "def get_set_subset_wv(s1_set, s2):\n",
    "    s2_list = get_words_ordered(s2.lower())\n",
    "    s2_list = [w for w in s2_list if w in model.wv]\n",
    "    if not s2_list:\n",
    "        return 0\n",
    "    s1_wv = model.wv[[w for w in s1_set if w in model.wv]]\n",
    "    s2_wv = model.wv[s2_list]\n",
    "    similarity_status = cosine_similarity(s1_wv, s2_wv)\n",
    "    return similarity_status.max(axis=1).sum()/len(s1_set)\n",
    "    \n",
    "\n",
    "def get_set_subset(s1_set, s2):\n",
    "    return get_set_subset_wv(s1_set, s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "metadata": {},
   "outputs": [],
   "source": [
    "User_text = \"good ambiance restaurants, serving fish\"  \n",
    "Location = \"Koramangala\"\n",
    "budget = 500\n",
    "cuisine_type = \"North Indian\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ambiance', 'fish', 'serving'}"
      ]
     },
     "execution_count": 751,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "User_text_refined = re.sub(r'(restaurants?|food|good|floor)', '', User_text.lower())\n",
    "User_text_set = get_words(User_text_refined)\n",
    "cuisine_type_set = get_words(cuisine_type)\n",
    "location_lower = \" \".join(get_words_ordered(Location))\n",
    "User_text_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendation_requirements_custom = recommendation_requirements\n",
    "if budget is not None :\n",
    "    recommendation_requirements_custom = recommendation_requirements_custom[\n",
    "        (recommendation_requirements_custom['cost'] <= budget) & (recommendation_requirements_custom['cost'] > 0)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_satisfied = recommendation_requirements_custom['location'].str.lower().apply(\n",
    "    lambda x: location_lower in x)\n",
    "if location_satisfied.any():\n",
    "    recommendation_requirements_custom = recommendation_requirements_custom[location_satisfied]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "metadata": {},
   "outputs": [],
   "source": [
    "cusine_req_fullfilled = recommendation_requirements_custom[\n",
    "    'cuisines_list'].apply(lambda x: cuisine_type in x)\n",
    "if cusine_req_fullfilled.any():\n",
    "    recommendation_requirements_custom = recommendation_requirements_custom[cusine_req_fullfilled]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "metadata": {},
   "outputs": [],
   "source": [
    "menu_item_scored = recommendation_requirements_custom['menu_item_evaled'].apply(lambda x: max(\n",
    "    [get_set_subset_standard(User_text_set, d) for d in x] + [0]))\n",
    "dish_liked_scored = recommendation_requirements_custom['dish_liked_list'].apply(lambda x: max(\n",
    "    [get_set_subset_standard(User_text_set, d) for d in x] + [0]))\n",
    "review_scores = recommendation_requirements_custom['reviews_list_evaled'].apply(lambda x: \n",
    "    [float_converter(r)/5*get_set_subset(User_text_set, t) for r,t in set(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 756,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def get_review_scores(scores):\n",
    "    num_review = len(scores)\n",
    "    if not num_review:\n",
    "        return 0\n",
    "    multiplier = 1/(1+math.exp(-((num_review-7)/2.8)))\n",
    "    return (sum(scores)/len(scores))*multiplier*(10/6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 757,
   "metadata": {},
   "outputs": [],
   "source": [
    "dish_score = dish_liked_scored*0.4 + menu_item_scored*0.2\n",
    "if dish_score.max() != 0:\n",
    "    dish_score = dish_score/dish_score.max()\n",
    "    recommendation_requirements_custom['final_score'] = dish_score*0.6 + \\\n",
    "        review_scores.apply(get_review_scores)*0.4\n",
    "else:\n",
    "    recommendation_requirements_custom['final_score'] = review_scores.apply(get_review_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>reviews_list_evaled</th>\n",
       "      <th>menu_item_evaled</th>\n",
       "      <th>cuisines_list</th>\n",
       "      <th>location</th>\n",
       "      <th>cost</th>\n",
       "      <th>rate</th>\n",
       "      <th>dish_liked_list</th>\n",
       "      <th>final_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1581</th>\n",
       "      <td>Paisa Vasool</td>\n",
       "      <td>[(Rated 4.0, RATEDn  It definitely goes by it'...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[North Indian]</td>\n",
       "      <td>Koramangala 6th Block</td>\n",
       "      <td>400</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[Fish Thali, Aloo Paratha]</td>\n",
       "      <td>0.635304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>Roti Wala</td>\n",
       "      <td>[(Rated 4.0, RATEDn  Well, I never had a chanc...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[North Indian]</td>\n",
       "      <td>Koramangala 5th Block</td>\n",
       "      <td>300</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[Gulab Jamun, Buttermilk, Egg Bhurji, Sprout S...</td>\n",
       "      <td>0.075898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1554</th>\n",
       "      <td>Bowlful</td>\n",
       "      <td>[(Rated 5.0, RATEDn  Ordered food to my office...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[North Indian]</td>\n",
       "      <td>Koramangala 4th Block</td>\n",
       "      <td>500</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.056398</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              name                                reviews_list_evaled  \\\n",
       "1581  Paisa Vasool  [(Rated 4.0, RATEDn  It definitely goes by it'...   \n",
       "875      Roti Wala  [(Rated 4.0, RATEDn  Well, I never had a chanc...   \n",
       "1554       Bowlful  [(Rated 5.0, RATEDn  Ordered food to my office...   \n",
       "\n",
       "     menu_item_evaled   cuisines_list               location  cost  rate  \\\n",
       "1581               []  [North Indian]  Koramangala 6th Block   400   3.0   \n",
       "875                []  [North Indian]  Koramangala 5th Block   300   4.0   \n",
       "1554               []  [North Indian]  Koramangala 4th Block   500   3.0   \n",
       "\n",
       "                                        dish_liked_list  final_score  \n",
       "1581                         [Fish Thali, Aloo Paratha]     0.635304  \n",
       "875   [Gulab Jamun, Buttermilk, Egg Bhurji, Sprout S...     0.075898  \n",
       "1554                                                 []     0.056398  "
      ]
     },
     "execution_count": 758,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommendation_requirements_custom.sort_values('final_score', ascending=False)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 744,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dish_score.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9089325807953956"
      ]
     },
     "execution_count": 745,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_scores.apply(get_review_scores).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 746,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 746,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dish_liked_scored.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 747,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_set_subset_standard(User_text_set, 'Chicken Masala Biryani,')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'biryani', 'chicken', 'masala'}"
      ]
     },
     "execution_count": 748,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_words('Chicken Masala Biryani,')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'authentic', 'biryani', 'chicken'}"
      ]
     },
     "execution_count": 749,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "User_text_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6666666666666667"
      ]
     },
     "execution_count": 702,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
